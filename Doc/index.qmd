---
title: AI and Productivity in Canada
author: Hans Martinez
date: today
format: pdf
bibliography: refs.bib
---

## Introduction

The large language model (LLM) ChatGPT debuted in November 2022, marking a significant milestone in the development of generative AI technologies. Since its introduction, generative AI has sparked widespread interest and debate regarding its potential to increase productivity at the individual and firm level.

What is the evidence of generative AI increasing productivity? A growing literature points to heterogeneous effects. In most cases, low-skilled workers seem to be the most benefited, but high-skilled workers could even decrease output or quality of their work. There is also evidence that AI could harm productivity. At the firm level, usage of AI —broadly defined— has been linked to higher productivity, but the literature is still scarce.

Low-skilled and less-experienced customer assistants improved significantly across different measures of productivity, after the introduction of a generative-AI based conversational assistant [@Brynjolfsson2023]. Access to the AI tool helped newer agents move across the experience curve more quickly. In contrast, there were minimal impacts on the productivity of more-skilled more-experienced agents. Importantly, the authors find evidence that AI assistance may decrease the quality of conversations by the most skilled agents.

In another experiment conducted at Ant Group, a large Chinese big tech company, researchers found similar results [@Gambacorta2024]. Following the introduction of CodeFuse, a large language model (LLM) designed to assist software programmer teams in coding, productivity increased by more than 50\%. Again, the higher gains in productivity were statistically significant only among entry-level and junior staff. When looking into why senior employees had a lower increase in productivity, the authors find that the senior employees used the gen AI less frequently.

For more complex tasks, such as code development, generative AI might reduce the output and quality of less-experienced users. Using observational data from 36,000 GitHub accounts, @Kreitmeir2024 documented that Italy's ban on ChatGPT increased output quantity and quality for less experienced users. Experienced users showed a decrease in more routine tasks. The authors argue that less-experienced users might take longer to identify and debug generative AI's faulty code.

Both of these effects —higher productivity gains for low-skilled workers and productivity decreases for complex tasks— are observed in the experiment conducted at Boston Consulting Group, a global management consulting firm [@DellAcqua2023]. In this experiment, researchers assigned consultants at random to three different conditions: no AI access, GPT-4 AI access, and GPT-4 AI access with prompt engineering overview. The authors find that consultants using AI were significantly more productive and produced higher quality results. Importantly, consultants below the average performance threshold increase by 43\% and those above increasing by 17\% compared to their owns scores. Furthermore, for complex tasks that the researchers considered to be outside the AI's current capabilities, consultants using AI were 19 percentage points less likely to produce correct solutions compared to those without AI.

At the individual level, these studies highlight two key points: First, AI expertise matters. General-purpose AI, when used for specialized tasks like coding or tasks beyond its capabilities, can reduce productivity by increasing error detection and correction time, as seen in the ChatGPT ban in Italy and the Boston Consulting Group experiment. Second, specialized AI significantly boosts productivity for low-skilled, less-experienced workers. This makes sense if the AI is trained on data generated by the top performers. This may be the case of the customer assistants. The AI could have been suggesting solutions the high skilled workers would take.

At the firm level, @Czarnitzki2023 finds a positive and significant association between the use of AI and firm productivity. The authors use a production function approach and survey data on German firms from 2019. Limited to a cross-section of German firms and a very small panel, the authors cannot use popular methods of production function estimation. Consequently, their study is limited to instrumental variables (IV) methods to account for the possible endogeneity arising from more productive firms being the ones investing more in AI. They employ as instruments —variables correlated with AI use but uncorrelated with productivity— the sector-level AI use, past innovation expenditure, and resistance to innovation by the firms' workers. Their data allows the researchers to measure AI usage with indicators (e.g. whether they use or not AI) and AI usage intensity (e.g. whether firms use AI in products/services, automation of processes, communication with customers, etc.). The authors define AI broadly because the 2019 survey lists only a few AI methods such as natural language understanding, image recognition, machine learning, and knowledge-based systems.

Finally, I could not find evidence or research work linking Canadian firms or workers with generative AI and productivity. However, Canada might be an interesting country in which to study the link between productivity and generative AI. This is due to Canada's diverse industry sectors, including a non-trivial professional, scientific and technical services. Additionally, understanding the impact of generative AI in Canada could provide valuable insights for policymakers and businesses aiming to close the productivity gap with other advanced economies.


### Open questions

Even though there is a growing interest in link between generative AI and productivity, there are still many open questions. In addition, there could be an area of opportunity for application in the Canadian market as I could not find evidence for Canadian firms or workers.

Among the possible research questions: 

- What is the evidence of the increase in labor productivity due to AI implementation in Canadian services firms? 
- What are the implementation strategies Canadian firms are taking to implement the use of AI by their employees? 
- What are the challenges firms are facing to have their employees use AI to increase their productivity? 
- What have been the unexpected negative effects of implementing AI technologies in the workplace? 
- What are the common uses of AI by the employees? 
- What are the commonly missed opportunities of using AI in the workplace? 
- What are the most common task employees use to automatize with AI?
- Had there been any significant differences between early vs late AI adopters in the workplace? 
- Are there any efforts from firms to level up late adopters?
- Are firms considering the heterogeneous effects on productivity by generative AI?
- In the long-term, will low-skilled workers develop their level of expertise, or will they keep relying on AI and hinder their expertise development? 

### Data

In terms of data, most individual-level studies rely on experimental data. An experiment could be an avenue for this paper, depending on the question we choose to tackle. The ideal experiment would involve a Canadian enterprise introducing AI assistance tools to its employees. This setup would allow us to control the treatment assignment (who gets access to AI tools) and measure the effects of AI on productivity. To explore the nuances of the effect of AI on productivity, the experiment will require measures of employees' skill levels and tasks with varying levels of difficulty.

Alternatively, an experiment can be conducted at Western University using students as participants. The experiment could involve coding exercises in various programming languages, ranging from popularly used ones like Python and R to less common ones like Fortran or Stata. Popular programming languages should represent a larger share of the gen AI training data set. It is expected that students with access to gen AI would perform better with popular languages such as Python and R, but their performance might decline with less familiar languages like Fortran or Stata. To account for skill levels, proxies such as students' grades in relevant courses could be used. We can also ask their familiarity with each language. The experiment could be conducted at the Ivey Behavioral Lab.

For firm-level data, Statistics Canada (StatCan) conducts several surveys that inquire about firms' use of AI, such as the Canadian Survey of Business Conditions (CSBC) and the Survey of Digital Technology and Internet Use (SDTIU). These surveys can potentially be linked by StatCan to other datasets they maintain, such as the Financial and Taxation Statistics for Enterprises (AFTS). Linking these datasets could help to establish a connection between AI usage and different measures of productivity. This linkage would allow for a more comprehensive analysis of how generative AI impacts firm-level productivity.

Access to the microdata of these surveys and linkages between datasets are restricted to the Research Data Centers (RDCs). There is one in the Social Science building in Western, however, obtaining access to this center and the datasets is a lengthy process.

## References {.appendix}

::: {#refs}
:::


## Appendix {.appendix}

{{< include sections/100-lit-rev.qmd >}}

{{< include sections/200-data.qmd >}}